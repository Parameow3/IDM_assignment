{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# News Classification Based on Their Headlines\n",
    "\n",
    "### This notebook outlines the process for classifying news based on their headlines using text mining and NLP techniques."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91889d7b646a3fef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8179d9bed05c069c"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T09:31:58.542160100Z",
     "start_time": "2024-01-14T09:31:57.381286800Z"
    }
   },
   "id": "53a36a23640051b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Load the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe84bf0319f3a218"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['TITLE', 'CATEGORY'], dtype='object')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace 'path_to_dataset.csv' with the actual file path\n",
    "dataframe = pd.read_csv('uci-news-aggregator.csv', encoding=\"utf8\", usecols=['TITLE', 'CATEGORY'])\n",
    "dataframe.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T09:31:59.087281200Z",
     "start_time": "2024-01-14T09:31:58.544167300Z"
    }
   },
   "id": "682881accb07260f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    TITLE CATEGORY\n0       Fed official says weak data caused by weather,...        b\n1       Fed's Charles Plosser sees high bar for change...        b\n2       US open: Stocks fall after Fed official hints ...        b\n3       Fed risks falling 'behind the curve', Charles ...        b\n4       Fed's Plosser: Nasty Weather Has Curbed Job Gr...        b\n...                                                   ...      ...\n422414  Surgeons to remove 4-year-old's rib to rebuild...        m\n422415  Boy to have surgery on esophagus after battery...        m\n422416  Child who swallowed battery to have reconstruc...        m\n422417  Phoenix boy undergoes surgery to repair throat...        m\n422418  Phoenix boy undergoes surgery to repair throat...        m\n\n[422419 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TITLE</th>\n      <th>CATEGORY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Fed official says weak data caused by weather,...</td>\n      <td>b</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Fed's Charles Plosser sees high bar for change...</td>\n      <td>b</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>US open: Stocks fall after Fed official hints ...</td>\n      <td>b</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Fed risks falling 'behind the curve', Charles ...</td>\n      <td>b</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n      <td>b</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>422414</th>\n      <td>Surgeons to remove 4-year-old's rib to rebuild...</td>\n      <td>m</td>\n    </tr>\n    <tr>\n      <th>422415</th>\n      <td>Boy to have surgery on esophagus after battery...</td>\n      <td>m</td>\n    </tr>\n    <tr>\n      <th>422416</th>\n      <td>Child who swallowed battery to have reconstruc...</td>\n      <td>m</td>\n    </tr>\n    <tr>\n      <th>422417</th>\n      <td>Phoenix boy undergoes surgery to repair throat...</td>\n      <td>m</td>\n    </tr>\n    <tr>\n      <th>422418</th>\n      <td>Phoenix boy undergoes surgery to repair throat...</td>\n      <td>m</td>\n    </tr>\n  </tbody>\n</table>\n<p>422419 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the dataset\n",
    "dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T09:31:59.094225100Z",
     "start_time": "2024-01-14T09:31:59.085281700Z"
    }
   },
   "id": "172806faae8b2e81"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16018c318d2254e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO missing data\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "#check for missing data\n",
    "if(any(dataframe.isnull().any())):\n",
    "    print('Missing Data\\n')\n",
    "    print(dataframe.isnull().sum())\n",
    "else:\n",
    "    print('NO missing data')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T09:31:59.116557400Z",
     "start_time": "2024-01-14T09:31:59.095229800Z"
    }
   },
   "id": "615cc0ace2370720"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows found\n",
      "Number of duplicate rows=  15112\n",
      "Dropping duplicates\n",
      "\n",
      "(407307, 2)\n"
     ]
    }
   ],
   "source": [
    "# check for duplicate\n",
    "if(any(dataframe.duplicated())==True):\n",
    "    print('Duplicate rows found')\n",
    "    print('Number of duplicate rows= ', dataframe[dataframe.duplicated()].shape[0])\n",
    "    dataframe.drop_duplicates(inplace=True,keep='first')\n",
    "    dataframe.reset_index(inplace=True,drop=True)\n",
    "    print('Dropping duplicates\\n')\n",
    "    print(dataframe.shape)\n",
    "else:\n",
    "    print('NO duplicate data')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T09:31:59.450741400Z",
     "start_time": "2024-01-14T09:31:59.113232400Z"
    }
   },
   "id": "32ef7c780c3c4955"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Parameow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Parameow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Parameow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the library to for the nltk functions to use in the cleaning process\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T09:32:00.216518Z",
     "start_time": "2024-01-14T09:31:59.445741500Z"
    }
   },
   "id": "658029a20e46a4d9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parameow\\anaconda3\\envs\\pythonProject\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Function for cleaning and tokenize the headline\n",
    "def tokenize(doc):\n",
    "    document = doc.lower()  # convert the content of the headline to lowercase\n",
    "    document = re.sub(r'\\d+', '',\n",
    "                      document)  # remove all of the digits inside of the content (using regular expressions)\n",
    "    document = document.translate(str.maketrans('', '', string.punctuation))  # remove the puntuations (, . ! # ...)\n",
    "    document = document.strip()  # remove the spaces at the start and end of the headline\n",
    "    return [wnl.lemmatize(token) for token in word_tokenize(document) if token not in stopwords.words('english')]\n",
    "    # tokenize the headlines\n",
    "    # and then filter only the words that are not in the english stopwords (words that are commonly used and give no benifits to the classifier)\n",
    "    # and finally lemmatize all of the tokens\n",
    "\n",
    "\n",
    "# The preprocess pipeline\n",
    "preprocessor = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),  # passing custom tokenizer method for the CountVectorizer to use\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "])\n",
    "\n",
    "X = dataframe[\"TITLE\"]\n",
    "Y = dataframe[\"CATEGORY\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    test_size = 0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "x_tfidf_train = preprocessor.fit_transform(X_train)  # process the training dataset\n",
    "x_tfidf_test = preprocessor.transform(X_test)  # process the test dataset\n",
    "# tfidf_test = preprocessor.transform(X_test.values) # process the testing dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T09:37:36.571971900Z",
     "start_time": "2024-01-14T09:32:00.210457Z"
    }
   },
   "id": "463cc06e019692d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8db131761b7b11ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Label encoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd65ed96424c911a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 3, 0, ..., 1, 0, 0])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_label_train = le.fit_transform(y_train)\n",
    "y_label_test = le.fit_transform(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T14:41:16.338337200Z",
     "start_time": "2024-01-14T14:41:16.310049600Z"
    }
   },
   "id": "73b6833daa5e4a0f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decision Tree Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e814d0063352a805"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score of Decision Tree:\n",
      "0.9011399998363245\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\")\n",
    "dt_classifier.fit(x_tfidf_train, y_label_train)\n",
    "dt_predictions = dt_classifier.predict(x_tfidf_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"accuracy score of Decision Tree:\")\n",
    "print(accuracy_score(y_label_test, dt_predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T09:41:00.747617600Z",
     "start_time": "2024-01-14T09:37:36.613205600Z"
    }
   },
   "id": "b07d8294a1b7241f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Multinomial Naive Bayes Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "895f1d339be72f1d"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score of Multinomial Naive Bayes:\n",
      "0.9206255677493801\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(x_tfidf_train, y_label_train)\n",
    "nb_predictions = nb_classifier.predict(x_tfidf_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"accuracy score of Multinomial Naive Bayes:\")\n",
    "print(accuracy_score(y_label_test, nb_predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T09:41:00.828232100Z",
     "start_time": "2024-01-14T09:41:00.749617Z"
    }
   },
   "id": "178a283226f7d318"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Artificial Neural Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff925052950d2d2e"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score of Artificial Neural Network:\n",
      "0.9483849320337499\n"
     ]
    }
   ],
   "source": [
    "nn_classifier = MLPClassifier()\n",
    "nn_classifier.fit(x_tfidf_train, y_label_train)\n",
    "nn_predictions = nn_classifier.predict(x_tfidf_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"accuracy score of Artificial Neural Network:\")\n",
    "print(accuracy_score(y_label_test, nn_predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T14:41:16.274049500Z",
     "start_time": "2024-01-14T09:41:00.830232100Z"
    }
   },
   "id": "1fb44284af7a5b8a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
